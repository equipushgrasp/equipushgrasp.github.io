<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EPG">
  <meta name="keywords" content="Robot Manipulation,Imitation Learning,Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EPG: Equipushgrasp</title>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true}, "HTML-CSS": {minScaleAdjust: 100} });
  </script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <link rel="icon" href="./static/images/icon.png" type="image/png">
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="https://heng-tian.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
            </a>
        </div>

    </div>
</nav>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Push-Grasp Policy Learning Using Equivariant Models and Grasp Score Optimization </h1>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                          <a href="https://bocehu.github.io/">Boce Hu</a><sup>$\star$</sup>,
                        </span>
                                    
                        <span class="author-block">
                          <a href="https://heng-tian.github.io/">Heng Tian</a><sup>$\star$</sup>,
                        </span>
                      
                        <span class="author-block">
                          <a href="https://www.dianwang.io/">Dian Wang</a>,
                        </span>
                                    
                        <span class="author-block">
                          <a href="https://haojhuang.github.io/">Haojie Huang</a>,
                        </span>
                                    
                        <span class="author-block">
                          <a href="https://zxp-s-works.github.io/">Xupeng Zhu</a>,
                        </span>
                                    
                        <span class="author-block">
                          <a href="https://www.robinwalters.com/">Robin Walters</a>,
                        </span>
                                    
                        <span class="author-block">
                          <a href="https://helpinghandslab.netlify.app/people/">Robert Platt</a>
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <div class="author-block">Northeastern University</div>
                        <div>
                            <span class="author-block"
                                  style="color: gray;"><sup>$\star$</sup>equal contribution</span>
                        </div>
                        <div><b>IEEE Robotics and Automation Letters (RAL) </b></div>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                              <a href="https://ieeexplore.ieee.org/document/11150764"
                                 class="external-link button is-normal is-rounded is-dark custom-button-text">
                                <span class="icon">
                                    <i class="fa-regular fa-file-lines"></i>
                                </span>
                                <span>Paper</span>
                              </a>
                            </span>

                            <!-- Video Link. -->
                            <span class="link-block">
                              <a href=""
                                 class="external-link button is-normal is-rounded is-dark custom-button-text">
                                <span class="icon">
                                    <i class="fab fa-youtube"></i>
                                </span>
                                <span>Video</span>
                              </a>
                            </span>
                            <!-- Tweet Link. -->
                            <span class="link-block">
                              <a href=""
                                      class="external-link button is-normal is-rounded is-dark custom-button-text">
                                <span class="icon">
                                    <i class="fa-brands fa-x-twitter"></i>
                                </span>
                                <span>Tweet</span>
                              </a>
                            </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                              <a href="https://github.com/equipushgrasp/Equipushgrasp"
                                      class="external-link button is-normal is-rounded is-dark custom-button-text">
                                <span class="icon">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                                </a>
                            </span>

                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">

                <div class="item item-demo1">
                    <video poster="" id="demo1" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/demo1.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo2">
                    <video poster="" id="demo2" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/demo2.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo3">
                    <video poster="" id="demo3" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/demo3.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-demo4">
                    <video poster="" id="demo4" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/demo4.mp4"
                                type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                         Goal-conditioned robotic grasping in cluttered environments remains a challenging
                         problem due to occlusions caused by surrounding objects, which prevent direct access 
                         to the target object. A promising solution to mitigate this issue is combining pushing 
                         and grasping policies, enabling active rearrangement of the scene to facilitate target 
                         retrieval. However, existing methods often overlook the rich geometric structures inherent 
                         in such tasks, thus limiting their effectiveness in complex, heavily cluttered scenarios. 
                         To address this, we propose the Equivariant Push-Grasp Network, a novel framework for 
                         joint pushing and grasping policy learning. Our contributions are twofold: 
                         (1) leveraging $SE(2)$-equivariance to improve both pushing and grasping performance and 
                         (2) a grasp score optimization-based training strategy that simplifies the joint learning process. 
                         Experimental results show that our method improves grasp success rates by 45%
                         in simulation and by 35% in real-world scenarios compared to strong baselines, 
                         representing a significant advancement in push-grasp policy learning. </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Summary of EPG</h2>
                <div class="content has-text-justified">
                    <p>
                        In this work, we introduce the $\textbf{Equivariant PushGrasp (EPG)}$ Network, 
                        a novel framework for efficient goal-conditioned push-grasp policy learning in 
                        cluttered environments. EPG leverages inherent task symmetries to improve both 
                        sample efficiency and performance. Specifically, we model the pushing and grasping 
                        policies using $SE(2)$-equivariant neural networks, embedding rotational and 
                        translational symmetry as an inductive bias. This design substantially enhances 
                        the model's generalization and data efficiency. Furthermore, we propose a 
                        self-supervised training approach that optimizes the pushing policy with a 
                        reward signal defined as the change in grasping scores before and after each push. 
                        This formulation simplifies the training procedure and naturally couples the learning 
                        of pushing and grasping.
                    </p>

                    <h4 class="title is-4 has-text-centered">Workflow of EPG</h4>
                    <p align="center">
                        <img src="./static/images/teaser.png" alt="" style="width:90%"></img>
                    </p>
                    <p>
                         The target object, specified by human instruction, is highlighted with a red mask (e.g., a banana). 
                         At each step, the push action direction is represented by an arrow. Our method iteratively predicts 
                         and executes push actions to create sufficient space for grasping the target. 
                         The final grasp pose is shown as a blue rectangle, with green blocks indicating the gripper's fingers.
                    </p>
                    <h4 class="title is-4 has-text-centered">Model Architecture of EPG</h4>
                    <p align="center">
                        <img src="./static/images/pipeline.png"></img>
                    </p>
                    <p>
                        Our model consists of three key components: a CriticNet, a GraspNet, and a PushNet. 
                        At each time step, GraspNet and PushNet generate a grasp action and a push action with 
                        respect to the target object. CriticNet then evaluates the grasp action by assigning it a score. 
                        If the score exceeds a predefined threshold $\tau$ or the maximum number of push attempts is reached,
                        the grasp action is executed. Otherwise, the push action is executed, and the process repeats with 
                        an updated observation.
                    </p>

                </div>
                <h4 class="title is-4">Two-Step Agent Learning</h4>
                <div class="content has-text-justified">

                    <p>
                        Compared to previous works that rely on complex alternating training between grasp and push networks, we propose a simple two-step training process. 
                        First we train a universal, goal-agnostic GraspNet together with a CriticNet that evaluates predicted grasps and returns a score. 
                        Then, we use the difference in grasp scores before and after pushing, computed from the CriticNet,
                        as a reward signal to train a goal-conditioned PushNet. This decoupled training strategy 
                        eliminates the need for alternating optimization and its scheduling-related hyperparameters, 
                        making the training more stable, controllable, and efficient. For more details, please refer to the paper.
                    </p>
                    <p align="center">
                        <img src="./static/images/train_pipeline.png" style="width:100%"></img>
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Experiments</h2>
                <div class="content has-text-justified">
                    <p>
                        We conduct experiments in both simulation and real-world environments to evaluate our method, with the setup
                        illustrated below. The evaluation consists of three tasks:
                    </p>
                    
                    <ul>
                        <li>
                            <strong>Goal-Conditioned Push-Grasp in Clutter:</strong>
                            This task assesses our framework's ability to retrieve a specific object from a cluttered scene.
                        </li>
                        <li>
                            <strong>Clutter Clearing:</strong>
                            This task evaluates the ability to clear an entire scene without any predefined target or grasp sequence.
                        </li>
                        <li>
                            <strong>Goal-Conditioned Push-Grasp in Tight Layouts:</strong>
                            In this task, objects are arranged in challenging geometric configurations (e.g., tight clusters, narrow gaps).
                            This is a hard task because the robot must push strategically to create graspable space in a constrained
                            environment.
                        </li>
                    </ul>

                </div>

                <div class="column is-centered">
                    <img src="./static/images/experiment_setup.png" style="width:55%"></img>
                </div>

                <div class="content has-text-centered has-text-justified">
                    <h4 class="title is-5 has-text-centered">
                        $\textbf{Simulation Experiment Result}$
                        
                    </h4>
                    <div class="column is-centered">
                        <img src="./static/images/table1.png" style="width:95%"></img>
                    </div>
                    <p>
                        The comparison result for the $\textbf{Goal-conditioned
                        Push-Grasp in Clutter task}$. Our method achieves the best performance, significantly outperforming all baselines.
                        On average, across all the settings with different number of
                        objects, it surpasses the best baseline by 44.7% in Grasp Success Rate (GSR).
                        The first two variations (Table I, row 3 and 4) show that integrating our approach into existing baselines further
                        improves their performance, which highlights our design’s effectiveness.
                    </p>

                    <div class="column is-centered">
                        <img src="./static/images/table2.png" style="width:95%"></img>
                    </div>
                    <p>
                        This table shows the results of the $\textbf{Clutter Clearing task}$.
                        Although this task is target-agnostic, push actions remain
                        beneficial in cluttered environments. Since there is no specific
                        target, we use the object with the highest score from GraspNet
                        as the target object for each step. The results show that our
                        method’s grasping capability exceeds all baselines by a large
                        margin in both with and without push actions.
                    </p>
                </div>


                <div class="content has-text-centered has-text-justified">
                    <h4 class="title is-5 has-text-centered">
                        $\textbf{Real-world Experiment Result}$
                
                    </h4>
                    <div class="columns is-centered">
                        <div class="column is-narrow has-text-centered">
                            <img src="./static/images/table4.png" style="width:45%;" />
                        </div>
                    </div>
                    <p>
                        This table presents the results of the real-world $\textbf{Goal-conditioned Push-Grasp in Clutter}$ task.
                        Our EPG significantly outperforms all baselines by at least
                        35% in GSR. The primary failure cases are: 1) inaccurate
                        object masks from SAM2, which further affect PushNet and
                        CriticNet outputs; 2) imprecise grasp poses predicted by the
                        GraspNet. Despite these challenges, our method demonstrates
                        strong overall stability.
                    </p>
                
                    <div class="columns is-centered">
                        <div class="column is-narrow has-text-centered">
                            <img src="./static/images/table3.png" style="width:35%;" />
                        </div>
                        
                    </div>

                    <div class="columns is-multiline is-mobile is-centered">
                        <div class="column is-one-eighth has-text-centered">
                            <img src="./static/images/Special_Case_Real/SC1.png" />
                            <p class="is-size-7">case1</p>
                        </div>
                        <div class="column is-one-eighth has-text-centered">
                            <img src="./static/images/Special_Case_Real/SC2.png" />
                            <p class="is-size-7">case2</p>
                        </div>
                        <div class="column is-one-eighth has-text-centered">
                            <img src="./static/images/Special_Case_Real/SC3.png" />
                            <p class="is-size-7">case3</p>
                        </div>
                        <div class="column is-one-eighth has-text-centered">
                            <img src="./static/images/Special_Case_Real/SC4.png" />
                            <p class="is-size-7">case4</p>
                        </div>
                        <div class="column is-one-eighth has-text-centered">
                            <img src="./static/images/Special_Case_Real/SC5.png" />
                            <p class="is-size-7">case5</p>
                        </div>
                        <div class="column is-one-eighth has-text-centered">
                            <img src="./static/images/Special_Case_Real/SC6.png" />
                            <p class="is-size-7">case6</p>
                        </div>
                        <div class="column is-one-eighth has-text-centered">
                            <img src="./static/images/Special_Case_Real/SC7.png" />
                            <p class="is-size-7">case7</p>
                        </div>
                        <div class="column is-one-eighth has-text-centered">
                            <img src="./static/images/Special_Case_Real/SC8.png" />
                            <p class="is-size-7">case8</p>
                        </div>
                    </div>

                    <p>
                        The configuration of the real-world $\textbf{Goal-Conditioned Push-Grasp in
                        Tight Layouts}$ task is shown above. It contains eight different
                        cases, each with a varying number of small boxes placed in
                        specific positions. The objective is to grasp the yellow box,
                        which is consistently placed at the center of surrounding boxes.
                        These tasks are unseen during training and require effective
                        strategies to solve, placing a strong demand on the generalization ability.
                        The results in this table indicate that despite increasing task
                        complexity, our method consistently outperforms the baselines
                        while maintaining stable performance.
                    </p>
                </div>

                <!-- <div class="columns is-centered">
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/videos/SC_1.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/videos/SC_4.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>
                    <div class="column is-centered">
                        <div class="video-container">
                            <video class="center" playsinline="" autoplay="" loop="" muted=""
                                   src="./static/videos/SC_7.mp4" style="border-radius:10px;"></video>

                        </div>
                    </div>
                </div> -->
            </div>
        </div>
    </div>
</section>


<!-- <section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">

                <div class="item item-SC1">
                    <video poster="" id="SC1" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/SC_1.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-SC2">
                    <video poster="" id="SC2" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/SC_2.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-SC3">
                    <video poster="" id="SC3" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/SC_3.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-SC4">
                    <video poster="" id="SC4" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/SC_4.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="item item-SC5">
                    <video poster="" id="SC5" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/SC_5.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-SC6">
                    <video poster="" id="SC6" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/SC_6.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-SC7">
                    <video poster="" id="SC7" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/SC_7.mp4"
                                type="video/mp4">
                    </video>
                </div>  
            </div>
        </div>
    </div>
</section> -->


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Video</h2>
                <video class="center" playsinline controls autoplay loop="" muted=""
                       src="./static/videos/SC_1.mp4" style="border-radius:10px;">
                </video>
                
            </div>
        </div>
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h3 class="title">Citation</h3>
        <pre><code>
@ARTICLE{
    11150764,
    author={Hu, Boce and Tian, Heng and Wang, Dian and Huang, Haojie and Zhu, Xupeng and Walters, Robin and Platt, Robert},
    journal={IEEE Robotics and Automation Letters},
    title={Push-Grasp Policy Learning Using Equivariant Models and Grasp Score Optimization},
    year={2025},
    doi={10.1109/LRA.2025.3606392}
}
    </code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">

        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        Website design borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
